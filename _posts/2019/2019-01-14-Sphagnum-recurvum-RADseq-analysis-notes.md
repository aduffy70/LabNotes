---
title:  Sphagnum recurvum RADseq analysis notes
date:  2019-01-14
layout: post
categories:
  - sphagnum
---

# Planning

Compared Blanka's ipyrad params file to my Crepidomanes intricatum and Carol Rowe's S. minima params files. Differences:

| Difference | Blanka / Wolf Lab | Notes |
| ---------- | ------------------- | ----- |
| restriction_overhang | AATT, TA / CAATTC, GTAA | The difference in the first overhang sequence probably makes no difference since ipyrad removes the barcode before looking for the overhang and doesn't require it to be at the very start of the sequence. The second overhang difference will cause problems in end-trimming. Carol and I verified that the one we used results in the cutadapt command generated by ipyrad searching for the exact sequence expected at the end of the reads based on the Gompert protocol (same sequence in old and new protocols). So using 'TA' would prevent many (most?) adapters from being trimmed. This could have been the cause of the data-churn problems Blanka adjusted for by increasing the Q-score cutoff. I am using the overhangs from the Wolf Lab runs. |
| max_low_quality_bases | 5 / 4  | Carol used 4 rather than the default 5. Reducing this will tend to shorten lower quality reads. It might be worth playing with to minimize the low-quality bases we expect with these 150bp reads where we expect garbage in the last third. Leaving at the default for now. I'll run fastq on the trimmed/filtered reads and decide whether to reduce it. |
| phred_Qscore_offset | 43 / 33  | Blanka increased this to require higher quality reads thinking that bad sequence at read ends was leading to "data-churn" but I suspect that might have been caused by untrimmed adapters. I am using the default 33 until I see evidence of a problem. |
| mindepth_statistical | 12 & 18 / 6 | This and mindepth_majority rule were both increased from the default (6). This will result in any locus with 6-11 or 6-17 reads in a sample getting thrown out, drastically reducing the number of loci. If we had mostly diploids, increasing these would reduce the number of heterozygotes miscalled as homozygotes, but with mostly haploids it is just throwing loci away unnecessarily. I'm going to put this back to the default. |
| max_barcode_mismatch | 1 / 0 or 2  | I don't know how many errors our barcodes were designed to allow, so I'll stick with the 1 that was used here. |
| max_alleles_consens | 1 / 2 | Setting it to 1 is appropriate for haploids but we have some diploids mixed in. At 1, all the heterozygous loci in the diploids will be discarded--more missing data for diploid samples. At 2, some loci with errors in haploids will be miscalled as heterozygous--which makes no sense for a haploid. Ideally, I would use all samples to find the loci, then cluster individuals and call genotypes in 2 batches--haploids and diploids--then bring everything back together before filtering loci by sample coverage. But I don't know for sure which are which ploidy level. I'll start by running as diploid and see if I can determine ploidy from heterozygosity levels. Then reconsider how to proceed. |
| trim_reads | 0,80,0,80 / 0,0,0,0 | Trimming the reads to a fixed length after quality trimming should reduce problems from the low-quality "extra third" readlengths. If we expect good reads to about bp100 we should be able to go beyond 80 though (100 - 8barcode = 92). I can check this was effective with FASTQC after trimming. |

# Process

## Demultiplex

Run each fastq and its barcode file through ipyrad step 1 separately to generate separate fastqs for each sample. Then we can run all the demultiplexed files through the rest of the process together.
